{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4198f581",
   "metadata": {},
   "source": [
    "## Univariate Linear regression\n",
    "Univariate linear regression is a simple and commonly used statistical technique for modeling the relationship between a dependent variable and an independent variable. The goal of univariate linear regression is to fit a line (a linear model) to the data that best represents the relationship between the dependent and independent variables. The line is represented by an equation, which can be used to predict the dependent variable given a specific value of the independent variable. Univariate linear regression can be applied when the relationship between the variables is assumed to be linear. The method involves finding the line of best fit, which is determined by minimizing the sum of the squares of the residuals (differences between the observed and predicted values of the dependent variable). The line of best fit can be used to make predictions and make inferences about the underlying relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a268a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Bangalore.csv')\n",
    "train_x = data.Area.to_numpy()/1000    # area as feature\n",
    "train_y = data.Price.to_numpy()/1000000    # price as target (converted to lakhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plotting raw data\n",
    "plt.scatter(train_x, train_y)\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price (Rupees)')\n",
    "plt.title(\"Bangalore's Real Estate\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b58d9e",
   "metadata": {},
   "source": [
    "### Prediction function\n",
    "We need to derive a function $f(w,b)$ where $f(w,b) = w*x + b$   \n",
    "here, $w$ is the weight and $b$ is the bias.  \n",
    "This function predicts the value of y (price) for a given x (floor size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% derived function\n",
    "def predicted_y(x):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        DESCRIPTION - x_train or x_test in this case\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : float\n",
    "        DESCRIPTION - predicted y based on x\n",
    "\n",
    "    '''\n",
    "    f = w*x + b\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2adbbf",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "In order to minimize our prediction error, a cost function is used,  \n",
    "$$J(w,b) = \\frac{1}{2m}\\sum_{i=0}^{m-1}  (f_{w,b}(x^i) - y^i)^2$$  \n",
    "This cost function calculates the prediction error - error between the predicted value and the training value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute cost\n",
    "def compute_cost(x,y,w_in,b_in):\n",
    "    '''\n",
    "    x : train_x\n",
    "    y : train_y\n",
    "    w_in,b_in : model parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost for a given w and b\n",
    "\n",
    "    '''\n",
    "    cost = 0\n",
    "    m = train_x.shape[0]\n",
    "    for i in range(m):\n",
    "        f_i = w_in*x[i] + b_in\n",
    "        cost += (f_i - y[i])**2\n",
    "    cost = cost/(2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3f3e9",
   "metadata": {},
   "source": [
    "### Compute gradient  \n",
    "Gradient is a partial derivative of the cost function $J(w,b)$ w.r.t. parameters $w$ and $b$\n",
    "$$\\frac{\\partial (J(w,b)}{\\partial (w)} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{w,b}(x^i) - y^i)x^i$$   \n",
    "$$\\frac{\\partial (J(w,b)}{\\partial (b)} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{w,b}(x^i) - y^i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute gradient\n",
    "def compute_gradient(x,y,w_in,b_in):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        DESCRIPTION - train_x\n",
    "    y : numpy.ndarray\n",
    "        DESCRIPTION - train_y\n",
    "    w_in : float\n",
    "        DESCRIPTION - weight w_in\n",
    "    y_in : float\n",
    "        DESCRIPTION - bias b\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradient dj_dw and dj_db\n",
    "\n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_i = w_in*x[i] + b_in\n",
    "        dj_dw_i = (f_i - y[i])*x[i]\n",
    "        dj_db_i = (f_i - y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "        \n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "#%%\n",
    "dj_dw, dj_dw = compute_gradient(train_x,train_y,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19ec0c",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "For every iteration the gradient is calculated, the values of $w$ and $b$ needs to be simultaneously updated.\n",
    "\n",
    "$$w = w - \\alpha \\frac{\\partial (J(w,b)}{\\partial (w)}$$\n",
    "$$b = b - \\alpha \\frac{\\partial (J(w,b)}{\\partial (b)}$$\n",
    "here, $\\alpha$ is the learning rate  \n",
    "This step needs to be repetaed until the values of $w$ and $b$ converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% gradient descent\n",
    "def gradient_descent(x,y,num_iters,alpha,w_in,b_in):\n",
    "    '''\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        DESCRIPTION - train_x\n",
    "    y : numpy.ndarray\n",
    "        DESCRIPTION - train_y\n",
    "    num_iters : int\n",
    "        DESCRIPTION - number of iterations\n",
    "    alpha : float\n",
    "        DESCRIPTION - learning rate\n",
    "    w_in : float\n",
    "        DESCRIPTION - weight w_in\n",
    "    y_in : float\n",
    "        DESCRIPTION - bias b.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w (scalar): Updated value of parameter after running gradient descent\n",
    "    b (scalar): Updated value of parameter after running gradient descent\n",
    "    J_history (List): History of cost values\n",
    "    p_history (list): History of parameters [w,b]\n",
    "\n",
    "    '''\n",
    "    \n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # compute gradient\n",
    "        dj_dw,dj_db = compute_gradient(x, y, w, b)\n",
    "        \n",
    "        # update parameters\n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "        # break the loop if the cost has converged i.e. the diference in cost is less than 1e-7\n",
    "        if  i>1000 and  (J_history[-1] - J_history[-2] <= 1e-7):    \n",
    "            print('\\ncost has converged')\n",
    "            break\n",
    "    print('w = {}\\nb = {}'.format(w,b))\n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% performing linear regression\n",
    "w_in = 0\n",
    "b_in = 0\n",
    "num_iters = 10000\n",
    "alpha = 1.0e-2\n",
    "\n",
    "# perform gradient descent\n",
    "w, b, J_history, p_history = gradient_descent(train_x, train_y, num_iters, alpha, w_in, b_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd108d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% performance\n",
    "f1 = plt.figure(1)\n",
    "f1.add_subplot(3,1,1)\n",
    "plt.plot(p_history[:][0],label = 'w')\n",
    "plt.legend()\n",
    "f1.add_subplot(3,1,2)\n",
    "plt.plot(p_history[:][1],label='b')\n",
    "plt.legend()\n",
    "f1.add_subplot(3,1,3)\n",
    "plt.plot(J_history,label = 'cost')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad294a5",
   "metadata": {},
   "source": [
    "In the above plot, although parameters w and b can be seen gradually reducing but the cost is already converged.\n",
    "Since it is not possible to predict the values accurately through a linear model, the cost will never be cloe to 0. So, as a simple rule, the cost is considered to be converged when the difference of cost in consequtive iterations becomes less than $1.0e-7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% fit\n",
    "plt.scatter(train_x,train_y,label = 'raw_data')  \n",
    "plt.plot(train_x,predicted_y(train_x),color = 'r',label='predicted price')\n",
    "plt.ylabel('Price (lakhs)') \n",
    "plt.xlabel('Floor Area')\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b60c08",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Linear regression is a statistical method used to model the linear relationship between a dependent variable and one or more independent variables. In this scenario, the data (area vs price) was used to derive a function that could be used to make predictions about the price of a property based on its area. A cost function was calculated and minimized using gradient descent to optimize the main prediction function. The final result is a linear regression model that can be used to make predictions about property prices based on their area."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
